{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA Binary Classification with a Bank Dataset","metadata":{}},{"cell_type":"markdown","source":"### This EDA refers to Kaggle playground competition: \"Binary Classification with a Bank Dataset\" \n- Competition link: https://www.kaggle.com/competitions/playground-series-s5e8\n- Main goal of this competition is to predict whether a client will subscribe to a bank term deposit.\n- Submissions are evaluated using ROC AUC between the predicted value and the observed target.\n- The dataset for this competition (both train and test) was generated from a deep learning model trained on the Bank Marketing Dataset dataset. Feature distributions are close to, but not exactly the same, as the original.\n- Start Date - August 1, 2025\n- Final Submission Deadline - August 31, 2025","metadata":{}},{"cell_type":"markdown","source":"### Labels details from original data set","metadata":{}},{"cell_type":"markdown","source":"- age: Age of the client (numeric)\n- job: Type of job (categorical: \"admin.\", \"blue-collar\", \"entrepreneur\", etc.)\n- marital: Marital status (categorical: \"married\", \"single\", \"divorced\")\n- education: Level of education (categorical: \"primary\", \"secondary\", \"tertiary\", \"unknown\")\n- default: Has credit in default? (categorical: \"yes\", \"no\")\n- balance: Average yearly balance in euros (numeric)\n- housing: Has a housing loan? (categorical: \"yes\", \"no\")\n- loan: Has a personal loan? (categorical: \"yes\", \"no\")\n- contact: Type of communication contact (categorical: \"unknown\", \"telephone\", \"cellular\")\n- day: Last contact day of the month (numeric, 1-31)\n- month: Last contact month of the year (categorical: \"jan\", \"feb\", \"mar\", ‚Ä¶, \"dec\")\n- duration: Last contact duration in seconds (numeric)\n- campaign: Number of contacts performed during this campaign (numeric)\n- pdays: Number of days since the client was last contacted from a previous campaign (numeric; -1 means the client was not previously contacted)\n- previous: Number of contacts performed before this campaign (numeric)\n- poutcome: Outcome of the previous marketing campaign (categorical: \"unknown\", \"other\", \"failure\", \"success\")\n- y: The target variable, whether the client subscribed to a term deposit","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom lightgbm import LGBMRegressor, LGBMClassifier\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.manifold import TSNE\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nwarnings.filterwarnings(\"ignore\",  category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning,)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:11:18.341349Z","iopub.execute_input":"2025-08-03T09:11:18.341632Z","iopub.status.idle":"2025-08-03T09:11:28.203066Z","shell.execute_reply.started":"2025-08-03T09:11:18.341569Z","shell.execute_reply":"2025-08-03T09:11:28.202280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s5e8/train.csv')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s5e8/test.csv')\nsub_df = pd.read_csv('/kaggle/input/playground-series-s5e8/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:22:17.208450Z","iopub.execute_input":"2025-08-03T09:22:17.208774Z","iopub.status.idle":"2025-08-03T09:22:19.200165Z","shell.execute_reply.started":"2025-08-03T09:22:17.208750Z","shell.execute_reply":"2025-08-03T09:22:19.199171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Basic check of data","metadata":{}},{"cell_type":"code","source":"display(train_df.head(4))\ndisplay(test_df.head(4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:11:31.348269Z","iopub.execute_input":"2025-08-03T09:11:31.348599Z","iopub.status.idle":"2025-08-03T09:11:31.389232Z","shell.execute_reply.started":"2025-08-03T09:11:31.348575Z","shell.execute_reply":"2025-08-03T09:11:31.388006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(train_df.info())\ndisplay(test_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:11:31.390392Z","iopub.execute_input":"2025-08-03T09:11:31.390739Z","iopub.status.idle":"2025-08-03T09:11:31.875239Z","shell.execute_reply.started":"2025-08-03T09:11:31.390706Z","shell.execute_reply":"2025-08-03T09:11:31.874309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(\"Summary for TRAIN data:\",train_df.describe())\ndisplay(\"Summary for TEST data:\",test_df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:11:31.876543Z","iopub.execute_input":"2025-08-03T09:11:31.876889Z","iopub.status.idle":"2025-08-03T09:11:32.179027Z","shell.execute_reply.started":"2025-08-03T09:11:31.876857Z","shell.execute_reply":"2025-08-03T09:11:32.178030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(\"TRAIN data\", train_df.nunique())\nprint('\\n')\ndisplay('TEST data',test_df.nunique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:11:32.180305Z","iopub.execute_input":"2025-08-03T09:11:32.180562Z","iopub.status.idle":"2025-08-03T09:11:32.718452Z","shell.execute_reply.started":"2025-08-03T09:11:32.180540Z","shell.execute_reply":"2025-08-03T09:11:32.717610Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- We have 7 numeric columns and 9 with categories\n- month column can be mapped to numerical column if needed\n- One coulmn with id in both train/test sets and additional column with labels in train set\n- number of unique values different for columns age, balance, duration, campaign, pdays, previous\n- min values different for train - test set for column duration\n- max values different balance, campaign, previous\n- pdays have two separate information - first number of days and second == -1 means the client was not previously contacted. Can be separated into two columns to separate category from numeric","metadata":{}},{"cell_type":"markdown","source":"### BONUS: Dimensions reduction with t-SNE","metadata":{}},{"cell_type":"code","source":"X = train_df.drop(columns=['id', 'y'])\ny = train_df['y']\n\n# Separation of numeric and categorical columns\ncat_cols = X.select_dtypes(include='object').columns.tolist()\nnum_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n\n# Pipeline: przetwarzanie cech\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), cat_cols)\n])\n\nX_processed = preprocessor.fit_transform(X)\n\n# Dimensions reduction with t-SNE\n# t-SNE is very slow for large datasets. We will check only 20k random samples\nSAMPLE_SIZE = 20000\n\nidx = np.random.choice(len(X_processed), SAMPLE_SIZE, replace=False)\nX_sample = X_processed[idx]\ny_sample = y.iloc[idx]\n\ntsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000, random_state=42)\nX_tsne = tsne.fit_transform(X_sample)\n\n\n# Creating plot\nplt.figure(figsize=(10, 8))\nsns.scatterplot(\n    x=X_tsne[:, 0],\n    y=X_tsne[:, 1],\n    hue=y_sample,\n#    palette='coolwarm',\n    s=5,\n    alpha=0.6,\n    linewidth=0\n)\nplt.title(\"t-SNE 2D wizualizacja danych (kolor = y)\")\nplt.xlabel(\"t-SNE 1\")\nplt.ylabel(\"t-SNE 2\")\nplt.legend(title='y')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:26:55.153762Z","iopub.execute_input":"2025-08-03T09:26:55.154077Z","iopub.status.idle":"2025-08-03T09:29:01.260984Z","shell.execute_reply.started":"2025-08-03T09:26:55.154052Z","shell.execute_reply":"2025-08-03T09:29:01.259999Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2nd BONUS - Separation of pdays and additional column wuth months as numeric","metadata":{}},{"cell_type":"code","source":"# we can create separate column with flag for -1 value\ntrain_df['no_previous_contact'] = (train_df['pdays'] == -1).astype(int)\ntest_df['no_previous_contact'] = (test_df['pdays'] == -1).astype(int)\n\n# We can create additional column with pdays only without -1 values\ntrain_df['pdays_cleaned'] = train_df['pdays'].where(train_df['pdays'] != -1, np.nan) \ntest_df['pdays_cleaned'] = test_df['pdays'].where(test_df['pdays'] != -1, np.nan) \n\n# We can create additional column with numeric months\ntrain_df['month_as_num'] = train_df['month'].map({'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11, 'dec':12})\ntest_df['month_as_num'] = test_df['month'].map({'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11, 'dec':12})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:11:32.719438Z","iopub.execute_input":"2025-08-03T09:11:32.719700Z","iopub.status.idle":"2025-08-03T09:11:32.818051Z","shell.execute_reply.started":"2025-08-03T09:11:32.719674Z","shell.execute_reply":"2025-08-03T09:11:32.817265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Duplicates and missing values check","metadata":{}},{"cell_type":"code","source":"print(\"Duplicates in TRAIN data:\", train_df.duplicated().sum())\nprint(\"Duplicates in TEST data:\", test_df.duplicated().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:11:32.818990Z","iopub.execute_input":"2025-08-03T09:11:32.819338Z","iopub.status.idle":"2025-08-03T09:11:33.629810Z","shell.execute_reply.started":"2025-08-03T09:11:32.819305Z","shell.execute_reply":"2025-08-03T09:11:33.629037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Missing values in TRAIN data:\\n\",train_df.isna().mean().apply(lambda x: f\"{x:.2%}\"))\nprint(\"\\nMissing values  in TEST data:\\n\",test_df.isna().mean().apply(lambda x: f\"{x:.2%}\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:11:33.632497Z","iopub.execute_input":"2025-08-03T09:11:33.632755Z","iopub.status.idle":"2025-08-03T09:11:34.126810Z","shell.execute_reply.started":"2025-08-03T09:11:33.632734Z","shell.execute_reply":"2025-08-03T09:11:34.125934Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- There is no duplicates in train/test data\n- There are no missing data in train/test sets\n- in case we decide to clean pdays we have missing 89,66% of data \n","metadata":{"execution":{"iopub.status.busy":"2025-07-30T07:33:20.111888Z","iopub.execute_input":"2025-07-30T07:33:20.112262Z","iopub.status.idle":"2025-07-30T07:33:20.293068Z","shell.execute_reply.started":"2025-07-30T07:33:20.112230Z","shell.execute_reply":"2025-07-30T07:33:20.291808Z"}}},{"cell_type":"markdown","source":"## 3. Train-Test drift check","metadata":{}},{"cell_type":"markdown","source":"### Numeric column drift","metadata":{"execution":{"iopub.status.busy":"2025-08-01T18:45:12.402360Z","iopub.execute_input":"2025-08-01T18:45:12.402651Z","iopub.status.idle":"2025-08-01T18:45:12.407030Z","shell.execute_reply.started":"2025-08-01T18:45:12.402623Z","shell.execute_reply":"2025-08-01T18:45:12.406070Z"}}},{"cell_type":"code","source":"for col in test_df.columns:\n    if col != 'id' and test_df[col].dtype in[np.int64,np.float64]:\n        sns.kdeplot(train_df[col], label='test', fill=True)\n        sns.kdeplot(test_df[col], label='train', fill=True)\n        plt.title(f\"Drift check for Column: {col}\")\n        plt.legend()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:11:34.614845Z","iopub.execute_input":"2025-08-03T09:11:34.615092Z","iopub.status.idle":"2025-08-03T09:12:17.281113Z","shell.execute_reply.started":"2025-08-03T09:11:34.615072Z","shell.execute_reply":"2025-08-03T09:12:17.280259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature ='balance'\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nupper_limit = train_df[feature].quantile(0.99)\n\n# Zoom \nsns.kdeplot(data=train_df, x=feature, ax=axes[0], label='train', fill=True, alpha=0.4)\nsns.kdeplot(data=test_df, x=feature, ax=axes[0], label='test', fill=True, alpha=0.4)\naxes[0].set_xlim(-1, upper_limit)\naxes[0].set_title('Data with zoom')\n\n# Full data\nsns.kdeplot(data=train_df, x=feature, ax=axes[1], label='train', fill=True, alpha=0.4)\nsns.kdeplot(data=test_df, x=feature, ax=axes[1], label='test', fill=True, alpha=0.4)\naxes[1].set_title('Full data')\n\nfor ax in axes:\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:12:17.282114Z","iopub.execute_input":"2025-08-03T09:12:17.282393Z","iopub.status.idle":"2025-08-03T09:12:26.126934Z","shell.execute_reply.started":"2025-08-03T09:12:17.282371Z","shell.execute_reply":"2025-08-03T09:12:26.125980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature ='duration'\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nupper_limit = train_df[feature].quantile(0.99)\n\n# Zoom \nsns.kdeplot(data=train_df, x=feature, ax=axes[0], label='train', fill=True, alpha=0.4)\nsns.kdeplot(data=test_df, x=feature, ax=axes[0], label='test', fill=True, alpha=0.4)\naxes[0].set_xlim(-1, upper_limit)\naxes[0].set_title('Data with zoom')\n\n# Full data\nsns.kdeplot(data=train_df, x=feature, ax=axes[1], label='train', fill=True, alpha=0.4)\nsns.kdeplot(data=test_df, x=feature, ax=axes[1], label='test', fill=True, alpha=0.4)\naxes[1].set_title('Full data')\n\nfor ax in axes:\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:12:26.128050Z","iopub.execute_input":"2025-08-03T09:12:26.128371Z","iopub.status.idle":"2025-08-03T09:12:34.792471Z","shell.execute_reply.started":"2025-08-03T09:12:26.128348Z","shell.execute_reply":"2025-08-03T09:12:34.791313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature ='campaign'\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nupper_limit = train_df[feature].quantile(0.99)\n\n# Zoom \nsns.kdeplot(data=train_df, x=feature, ax=axes[0], label='train', fill=True, alpha=0.4)\nsns.kdeplot(data=test_df, x=feature, ax=axes[0], label='test', fill=True, alpha=0.4)\naxes[0].set_xlim(-1, upper_limit)\naxes[0].set_title('Data with zoom')\n\n# Full data\nsns.kdeplot(data=train_df, x=feature, ax=axes[1], label='train', fill=True, alpha=0.4)\nsns.kdeplot(data=test_df, x=feature, ax=axes[1], label='test', fill=True, alpha=0.4)\naxes[1].set_title('Full data')\n\nfor ax in axes:\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:12:34.793481Z","iopub.execute_input":"2025-08-03T09:12:34.793772Z","iopub.status.idle":"2025-08-03T09:12:43.340869Z","shell.execute_reply.started":"2025-08-03T09:12:34.793750Z","shell.execute_reply":"2025-08-03T09:12:43.339966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature ='pdays'\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nupper_limit = train_df[feature].quantile(0.9)\n\n# Zoom \nsns.kdeplot(data=train_df, x=feature, ax=axes[0], label='train', fill=True, alpha=0.4)\nsns.kdeplot(data=test_df, x=feature, ax=axes[0], label='test', fill=True, alpha=0.4)\naxes[0].set_xlim(-1, upper_limit)\naxes[0].set_title('Data with zoom')\n\n# Full data\nsns.kdeplot(data=train_df, x=feature, ax=axes[1], label='train', fill=True, alpha=0.4)\nsns.kdeplot(data=test_df, x=feature, ax=axes[1], label='test', fill=True, alpha=0.4)\naxes[1].set_title('Full data')\n\nfor ax in axes:\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:12:43.341816Z","iopub.execute_input":"2025-08-03T09:12:43.342066Z","iopub.status.idle":"2025-08-03T09:12:52.134114Z","shell.execute_reply.started":"2025-08-03T09:12:43.342047Z","shell.execute_reply":"2025-08-03T09:12:52.133243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature ='previous'\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nupper_limit = train_df[feature].quantile(0.999)\n\n# Zoom \nsns.kdeplot(data=train_df, x=feature, ax=axes[0], label='train', fill=True, alpha=0.4)\nsns.kdeplot(data=test_df, x=feature, ax=axes[0], label='test', fill=True, alpha=0.4)\naxes[0].set_xlim(-1, upper_limit)\naxes[0].set_title('Data with zoom')\n\n# Full data\nsns.kdeplot(data=train_df, x=feature, ax=axes[1], label='train', fill=True, alpha=0.4)\nsns.kdeplot(data=test_df, x=feature, ax=axes[1], label='test', fill=True, alpha=0.4)\naxes[1].set_title('Full data')\n\nfor ax in axes:\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:12:52.135245Z","iopub.execute_input":"2025-08-03T09:12:52.135558Z","iopub.status.idle":"2025-08-03T09:13:00.664717Z","shell.execute_reply.started":"2025-08-03T09:12:52.135534Z","shell.execute_reply":"2025-08-03T09:13:00.663766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Categorical columns drift check","metadata":{}},{"cell_type":"code","source":"def plot_category_drift(feature):\n    pd.concat([\n        train_df[feature].value_counts(normalize=True).rename(\"train\"),\n        test_df[feature].value_counts(normalize=True).rename(\"test\")\n    ], axis=1).plot(kind=\"bar\", title=f\"Category drift: {feature}\")\n\ncolumns = [ 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact','month', 'poutcome'  ]\n\nfor col in columns:\n    plot_category_drift(col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:13:00.665653Z","iopub.execute_input":"2025-08-03T09:13:00.665878Z","iopub.status.idle":"2025-08-03T09:13:03.497162Z","shell.execute_reply.started":"2025-08-03T09:13:00.665861Z","shell.execute_reply":"2025-08-03T09:13:03.496173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- visible drift for column 'previous'\n- no clear drift between train and test data for rest of numeric columns\n- no clear drift between train and test data for categorical columns","metadata":{}},{"cell_type":"markdown","source":"## 4. Correlation check for train and test data","metadata":{}},{"cell_type":"code","source":"sns.heatmap(train_df[['age', 'balance','day', 'duration','campaign', 'pdays','previous', 'month_as_num', 'pdays_cleaned']].corr(),\n            annot = True, cmap='coolwarm')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:13:03.498152Z","iopub.execute_input":"2025-08-03T09:13:03.498442Z","iopub.status.idle":"2025-08-03T09:13:04.167713Z","shell.execute_reply.started":"2025-08-03T09:13:03.498421Z","shell.execute_reply":"2025-08-03T09:13:04.166710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.heatmap(test_df[['age', 'balance','day', 'duration','campaign', 'pdays','previous','month_as_num', 'pdays_cleaned']].corr(), \n            annot = True, cmap='coolwarm')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:13:04.168687Z","iopub.execute_input":"2025-08-03T09:13:04.168989Z","iopub.status.idle":"2025-08-03T09:13:04.690926Z","shell.execute_reply.started":"2025-08-03T09:13:04.168960Z","shell.execute_reply":"2025-08-03T09:13:04.689981Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- Only pdays and previous columns show strong correlation\n- possible weak negative correlation between new columns - month_as_num and pdays_cleaned","metadata":{}},{"cell_type":"markdown","source":"## 5. Check of 'y' in each column for train data","metadata":{}},{"cell_type":"markdown","source":"### Category columns","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=train_df, x='y')\nround(train_df['y'].value_counts(normalize=True)*100,2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:13:04.691633Z","iopub.execute_input":"2025-08-03T09:13:04.691861Z","iopub.status.idle":"2025-08-03T09:13:04.878516Z","shell.execute_reply.started":"2025-08-03T09:13:04.691844Z","shell.execute_reply":"2025-08-03T09:13:04.877316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = [ 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',  'poutcome', \n            'month_as_num', 'no_previous_contact']\nfor col in columns:\n    train_df.groupby([col,'y']).size().unstack().plot(kind='bar', stacked=True, title=col)\n    plt.show()\n    print('Percentage summary:')\n    display((pd.crosstab(train_df[col], train_df[\"y\"], normalize='index') * 100).round(1))\n    print('Quantitative summary:')\n    display((pd.crosstab(train_df[col], train_df[\"y\"])))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:17:17.998483Z","iopub.execute_input":"2025-08-03T09:17:17.998774Z","iopub.status.idle":"2025-08-03T09:17:22.097794Z","shell.execute_reply.started":"2025-08-03T09:17:17.998753Z","shell.execute_reply":"2025-08-03T09:17:22.097012Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- We have 2 gropus in label column with split 88% 12% - strong unbalance of classes\n- Each categorical column show caategories with different ratio of 0 / 1 labels and have potential to be used in classification\n","metadata":{}},{"cell_type":"markdown","source":"### Numerical columns","metadata":{}},{"cell_type":"code","source":"columns = [ 'age','balance', 'day', 'duration', 'campaign', 'pdays', 'previous',  'pdays_cleaned', 'month_as_num']\n\nfor feature in columns:\n    plt.figure(figsize=(8, 4))\n    sns.kdeplot(data=train_df[train_df['y'] == 0], x=feature, label='y = 0', fill=True, alpha=0.4)\n    sns.kdeplot(data=train_df[train_df['y'] == 1], x=feature, label='y = 1', fill=True, alpha=0.4)\n    plt.title(f'KDE for {feature}')\n    #plt.xscale('log')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:13:09.003885Z","iopub.execute_input":"2025-08-03T09:13:09.004132Z","iopub.status.idle":"2025-08-03T09:13:37.699984Z","shell.execute_reply.started":"2025-08-03T09:13:09.004114Z","shell.execute_reply":"2025-08-03T09:13:37.699036Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- Most of columns show differences in density for labels 0 an 1 and have good potential to be used for classification - age, day, duration, pdays_cleaned, month_as_num\n- month column seems to not show cyclical behaviour so it can be probably used as categorical column\n- day seems to have no cyclical behaviour so it can be also consider as categorical however 31 categories can be quite large number - to be checked\n- new column pdays_cleaned show differences between 0 and 1 but it consider about 11% of data, rest of them is -1\n- some plots are not so godd visible so they can be analysed in more details like previous","metadata":{}},{"cell_type":"markdown","source":"### Additional plots for more details","metadata":{}},{"cell_type":"code","source":"feature ='balance'\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nupper_limit = train_df[feature].quantile(0.99)\n\n# Zoom\nsns.kdeplot(data=train_df[train_df['y'] == 0], x=feature, ax=axes[0], label='y = 0', fill=True, alpha=0.4)\nsns.kdeplot(data=train_df[train_df['y'] == 1], x=feature, ax=axes[0], label='y = 1', fill=True, alpha=0.4)\naxes[0].set_xlim(0, upper_limit)\naxes[0].set_title(f'Zoom for {feature}')\n\n# Full\nsns.kdeplot(data=train_df[train_df['y'] == 0], x=feature, ax=axes[1], label='y = 0', fill=True, alpha=0.4)\nsns.kdeplot(data=train_df[train_df['y'] == 1], x=feature, ax=axes[1], label='y = 1', fill=True, alpha=0.4)\naxes[1].set_title(f'Full data for {feature}')\n\nfor ax in axes:\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:13:37.700839Z","iopub.execute_input":"2025-08-03T09:13:37.701094Z","iopub.status.idle":"2025-08-03T09:13:44.633267Z","shell.execute_reply.started":"2025-08-03T09:13:37.701073Z","shell.execute_reply":"2025-08-03T09:13:44.632279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature ='duration'\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nupper_limit = train_df[feature].quantile(0.995)\n\n# Zoom\nsns.kdeplot(data=train_df[train_df['y'] == 0], x=feature, ax=axes[0], label='y = 0', fill=True, alpha=0.4)\nsns.kdeplot(data=train_df[train_df['y'] == 1], x=feature, ax=axes[0], label='y = 1', fill=True, alpha=0.4)\naxes[0].set_xlim(0, upper_limit)\naxes[0].set_title(f'Zoom for {feature}')\n\n# Full\nsns.kdeplot(data=train_df[train_df['y'] == 0], x=feature, ax=axes[1], label='y = 0', fill=True, alpha=0.4)\nsns.kdeplot(data=train_df[train_df['y'] == 1], x=feature, ax=axes[1], label='y = 1', fill=True, alpha=0.4)\naxes[1].set_title(f'Full data for {feature}')\n\nfor ax in axes:\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:13:44.634310Z","iopub.execute_input":"2025-08-03T09:13:44.634803Z","iopub.status.idle":"2025-08-03T09:13:51.591802Z","shell.execute_reply.started":"2025-08-03T09:13:44.634767Z","shell.execute_reply":"2025-08-03T09:13:51.590730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature ='campaign'\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nupper_limit = train_df[feature].quantile(0.97)\n\n# Zoom\nsns.kdeplot(data=train_df[train_df['y'] == 0], x=feature, ax=axes[0], label='y = 0', fill=True, alpha=0.4)\nsns.kdeplot(data=train_df[train_df['y'] == 1], x=feature, ax=axes[0], label='y = 1', fill=True, alpha=0.4)\naxes[0].set_xlim(0, upper_limit)\naxes[0].set_title(f'Zoom  for {feature}')\n\n# Full\nsns.kdeplot(data=train_df[train_df['y'] == 0], x=feature, ax=axes[1], label='y = 0', fill=True, alpha=0.4)\nsns.kdeplot(data=train_df[train_df['y'] == 1], x=feature, ax=axes[1], label='y = 1', fill=True, alpha=0.4)\naxes[1].set_title(f'Full data for {feature}')\n\nfor ax in axes:\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:13:51.592773Z","iopub.execute_input":"2025-08-03T09:13:51.593043Z","iopub.status.idle":"2025-08-03T09:13:58.463776Z","shell.execute_reply.started":"2025-08-03T09:13:51.593023Z","shell.execute_reply":"2025-08-03T09:13:58.462798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature ='previous'\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nupper_limit = train_df[feature].quantile(0.995)\n\n# Zoom\nsns.kdeplot(data=train_df[train_df['y'] == 0], x=feature, ax=axes[0], label='y = 0', fill=True, alpha=0.4)\nsns.kdeplot(data=train_df[train_df['y'] == 1], x=feature, ax=axes[0], label='y = 1', fill=True, alpha=0.4)\naxes[0].set_xlim(0, upper_limit)\naxes[0].set_title(f'Zoom for {feature}')\n\n# Full\nsns.kdeplot(data=train_df[train_df['y'] == 0], x=feature, ax=axes[1], label='y = 0', fill=True, alpha=0.4)\nsns.kdeplot(data=train_df[train_df['y'] == 1], x=feature, ax=axes[1], label='y = 1', fill=True, alpha=0.4)\naxes[1].set_title(f'Full data for {feature}')\n\nfor ax in axes:\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:13:58.464724Z","iopub.execute_input":"2025-08-03T09:13:58.464976Z","iopub.status.idle":"2025-08-03T09:14:05.316486Z","shell.execute_reply.started":"2025-08-03T09:13:58.464955Z","shell.execute_reply":"2025-08-03T09:14:05.315163Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- Balance column show small differences in density - can be verified with feature importance\n- duration column show quite good differences in density - it is good potential for classification\n- campaign column seems to have no bigger differences -  can be verified with feature importance\n- for column previous KDE seems to show much different behaviour and seems to have a very good potential for classification","metadata":{}},{"cell_type":"markdown","source":"## 6. Outliners for numerical columns","metadata":{}},{"cell_type":"code","source":"columns = [ 'age','balance', 'day', 'duration', 'campaign', 'pdays', 'previous',  'pdays_cleaned', 'month_as_num']\n\nfor col in columns:\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(x='y', y=col, data=train_df)\n    plt.title(f'Boxplot of {col} by y label column')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:14:05.322521Z","iopub.execute_input":"2025-08-03T09:14:05.322825Z","iopub.status.idle":"2025-08-03T09:14:08.082318Z","shell.execute_reply.started":"2025-08-03T09:14:05.322801Z","shell.execute_reply":"2025-08-03T09:14:08.081312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## t_SNE lub PCA ???? ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:14:08.083327Z","iopub.execute_input":"2025-08-03T09:14:08.083665Z","iopub.status.idle":"2025-08-03T09:14:08.087915Z","shell.execute_reply.started":"2025-08-03T09:14:08.083634Z","shell.execute_reply":"2025-08-03T09:14:08.087028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.countplot(data=train_df,  x='housing', hue='y')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:14:08.088862Z","iopub.execute_input":"2025-08-03T09:14:08.089247Z","iopub.status.idle":"2025-08-03T09:14:08.747084Z","shell.execute_reply.started":"2025-08-03T09:14:08.089194Z","shell.execute_reply":"2025-08-03T09:14:08.746064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(data=train_df, x='education', y='pdays_cleaned')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:14:08.748104Z","iopub.execute_input":"2025-08-03T09:14:08.748440Z","iopub.status.idle":"2025-08-03T09:14:09.209065Z","shell.execute_reply.started":"2025-08-03T09:14:08.748411Z","shell.execute_reply":"2025-08-03T09:14:09.207864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.boxplot(data=train_df, x='marital', y='age', hue='y')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:14:09.210005Z","iopub.execute_input":"2025-08-03T09:14:09.210275Z","iopub.status.idle":"2025-08-03T09:14:10.052594Z","shell.execute_reply.started":"2025-08-03T09:14:09.210251Z","shell.execute_reply":"2025-08-03T09:14:10.051436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.pairplot(train_df[[ 'age','balance', 'pdays_cleaned','y']], hue='y') ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:14:10.053599Z","iopub.execute_input":"2025-08-03T09:14:10.053858Z","iopub.status.idle":"2025-08-03T09:16:31.372930Z","shell.execute_reply.started":"2025-08-03T09:14:10.053836Z","shell.execute_reply":"2025-08-03T09:16:31.371608Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"üîß 1. Przekszta≈Çcenia kolumn kategorycznych\na) One-Hot Encoding / Target Encoding\nDla zmiennych takich jak: job, marital, education, contact, month, poutcome.\n\nJe≈õli korzystasz z modeli drzewiastych (np. XGBoost, LightGBM), mo≈ºesz u≈ºyƒá Target Encoding.\n\nDla regresji logistycznej czy SVM ‚Äì One-Hot Encoding.\n\nb) Grupowanie kategorii\njob: Mo≈ºesz pogrupowaƒá zawody wg dochodu, statusu spo≈Çecznego lub stabilno≈õci zatrudnienia.\n\neducation: Po≈ÇƒÖczyƒá unknown z primary lub stworzyƒá grupƒô \"unknown\" osobno ‚Äì sprawdziƒá korelacjƒô z targetem.\n\nmonth: Zamieniƒá na numer miesiƒÖca (jan = 1 itd.) i/lub pogrupowaƒá na kwarta≈Çy lub sezony (wiosna/lato/jesie≈Ñ/zima).\n\npoutcome: Bardzo wa≈ºna zmienna ‚Äì warto zostawiƒá jako osobnƒÖ kolumnƒô, ale te≈º stworzyƒá zmiennƒÖ binarnƒÖ typu: prev_success = poutcome == 'success'.\n\nüßÆ 2. Przekszta≈Çcenia kolumn liczbowych\na) Standaryzacja / Normalizacja\nDla modeli wra≈ºliwych na skalƒô (regresja logistyczna, SVM) warto przeskalowaƒá: age, balance, duration, campaign, pdays, previous.\n\nb) Transformacje logarytmiczne / winsoryzacja\nbalance, duration, pdays, previous mogƒÖ mieƒá rozk≈Çad z ogonami ‚Äì warto sprawdziƒá wykresy. Spr√≥buj:\n\nlog(1 + balance), log(1 + duration), log(1 + previous)\n\nWinsoryzacja (obciƒôcie ekstremalnych warto≈õci)\n\nüß† 3. Tworzenie nowych cech (Feature Engineering)\na) Interakcje miƒôdzy zmiennymi\nage * duration, job + education, contact * month, poutcome * previous ‚Äì interakcje mogƒÖ ujawniaƒá niuanse kampanii.\n\nStw√≥rz kolumnƒô recently_contacted = pdays != -1 jako cecha binarna.\n\nb) Grupowanie wieku\nage_group = 'young' (<=30) / 'middle' (31‚Äì60) / 'senior' (>60) ‚Äì mo≈ºe mieƒá r√≥≈ºny wp≈Çyw na decyzje kredytowe.\n\nc) Zmienna sezonowa\nis_summer = month in ['jun', 'jul', 'aug'] ‚Äì kampanie w wakacje mogƒÖ mieƒá innƒÖ skuteczno≈õƒá.\n\nd) D≈Çugo≈õƒá kontaktu / skuteczno≈õƒá kontaktu\nlong_contact = duration > X (np. 120 sekund)\n\ncampaign_efficiency = previous / (1 + campaign) ‚Äì czy wiele kontakt√≥w wcze≈õniej przynosi≈Ço efekt?\n\nüîç 4. Sprawdzenie korelacji / wa≈ºno≈õci cech\nWykorzystaj:\n\nFeature Importance z modelu drzewiastego.\n\nPermutation Importance\n\nSHAP values ‚Äì do zrozumienia wp≈Çywu cech na predykcjƒô.\n\nüß™ 5. Eksperymenty i walidacja\na) Sprawd≈∫:\nCzy duration nie przecieka informacji? (je≈õli znana tylko po kampanii ‚Äì mo≈ºe trzeba jƒÖ pominƒÖƒá).\n\nCzy pdays == -1 to tylko brak kontaktu, czy te≈º dodatkowa informacja o ‚Äú≈õwie≈ºym‚Äù kliencie?\n\nJaki jest rozk≈Çad targetu y ‚Äì czy masz problem imbalance? Je≈õli tak, rozwa≈º:\n\nSMOTE, undersampling majority class, class_weight.\n\n‚úÖ Przyk≈Çadowe cechy do przetestowania:\nNowa kolumna\tOpis\nage_group\tkategoryczna: young / middle / senior\nlog_balance\tlog(1 + balance)\ncontacted_before\tbinary: pdays != -1\nseason\tcategorical: winter/spring/summer/fall\ncontact_efficiency\tprevious / (1 + campaign)\nlong_contact\tbinary: duration > 120\neducation_known\tbinary: education != 'unknown'","metadata":{}},{"cell_type":"markdown","source":"1. Czasowe i kampanijne zale≈ºno≈õci\n‚è±Ô∏è ‚ÄûD≈Çugo≈õƒá kontaktu‚Äù / ‚Äûliczba kontakt√≥w‚Äù\nkontakt_avg_time = duration / campaign ‚Äì ≈õredni czas kontaktu na jeden kontakt\n\nüìâ Trend kontakt√≥w\ndelta_contact = previous - campaign ‚Äì czy liczba kontakt√≥w wzros≈Ça czy spad≈Ça\n\nhas_previous_contact = (pdays != 999).astype(int) ‚Äì flaga, czy kontakt by≈Ç wcze≈õniej\n\nüìÖ Sezonowo≈õƒá\nZmienna month ‚Äì zakoduj jƒÖ jako int (np. Jan = 1) i dodaj zmiennƒÖ:\n\nis_q4_campaign = month.isin(['oct', 'nov', 'dec']) ‚Äì mo≈ºe zimƒÖ skuteczno≈õƒá spada?\n\nüßë‚Äçüíº 2. Zachowanie klienta i profil demograficzny\nüí≥ ZobowiƒÖzania finansowe\nloan_sum = (housing == 'yes') + (loan == 'yes') ‚Äì liczba aktywnych po≈ºyczek\n\nis_deep_debt = (balance < 0) & (loan_sum > 1) ‚Äì mocno zad≈Çu≈ºony\n\nüß† Poziom edukacji + zaw√≥d\nedu_job = education + \"_\" + job ‚Äì np. \"tertiary_admin\"\n\nMo≈ºna zakodowaƒá i u≈ºyƒá jako cechy (one-hot lub target encoding)\n\nüë´ Ma≈Ç≈ºe≈Ñstwo vs. wiek\nis_young_single = (age < 30) & (marital == 'single')\n\nis_old_married = (age > 60) & (marital == 'married')\n\nüìû 3. Komunikacja i kana≈Ç kontaktu\nüîî Efektywno≈õƒá kana≈Çu kontaktu\ncontact_success_rate = campaign / (duration + 1) ‚Äì ile kontakt√≥w na minutƒô\n\nüõ†Ô∏è Rodzaj komunikacji + sukces poprzedni\nchannel_prev_success = contact + \"_\" + poutcome\n\n\n\nüß™ Narzƒôdzia do wykrywania nieliniowych zale≈ºno≈õci:\npd.plotting.scatter_matrix() ‚Äì szybki rzut oka\n\nsns.pairplot() ‚Äì dla ma≈Çej liczby kolumn\n\nsklearn.feature_selection.mutual_info_classif(X, y) ‚Äì mierzy nieliniowƒÖ zale≈ºno≈õƒá","metadata":{}},{"cell_type":"markdown","source":"1. Numeryczne ‚Üî Numeryczne\nPairplot (sns.pairplot)\nKilka zmiennych naraz ‚Äì przeglƒÖd zale≈ºno≈õci i gƒôsto≈õci\nsns.pairplot(df, hue='label')  # opcjonalnie hue dla klasy binarnej\n\nScatterplot z hue / style / size\nSuper do odkrywania nieliniowych relacji\nsns.scatterplot(data=df, x='age', y='balance', hue='loan', style='marital')\n\nHeatmap korelacji (sns.heatmap)\nPokazuje mocne lub s≈Çabe powiƒÖzania liniowe\nsns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')\n\n2. Kategoryczne ‚Üî Numeryczne\nBoxplot (sns.boxplot)\nWidaƒá mediany, rozrzut i outliery\nsns.boxplot(data=df, x='education', y='balance')\n\nViolinplot (sns.violinplot)\nTo samo co boxplot, ale z KDE ‚Äì pokazuje lepiej gƒôsto≈õci\nsns.violinplot(data=df, x='job', y='age', hue='y', split=True)\n\nSwarmplot (sns.swarmplot)\nKa≈ºdy punkt osobno ‚Äì dobry na ma≈Çych zbiorach\nsns.swarmplot(data=df, x='marital', y='balance')\n\nBarplot (sns.barplot)\n≈örednia warto≈õƒá cechy numerycznej dla kategorii (np. ≈õredni balance dla job)\nsns.barplot(data=df, x='job', y='balance')\n\n 3. Kategoryczne ‚Üî Kategoryczne\nHeatmap cross-tab (pd.crosstab + sns.heatmap)\nIle wystƒÖpie≈Ñ danego po≈ÇƒÖczenia\nct = pd.crosstab(df['job'], df['marital'])\nsns.heatmap(ct, annot=True, fmt='d', cmap='Blues')\n\nCountplot (sns.countplot)\nLiczba obserwacji w ka≈ºdej kategorii + hue np. y\nsns.countplot(data=df, x='education', hue='y')\n\n4. Numeryczne ‚Üî Target binarny\nKDE Plot (sns.kdeplot)\nRozk≈Çad cechy w dw√≥ch klasach\nsns.kdeplot(data=df[df['y'] == 0], x='age', label='No Loan')\nsns.kdeplot(data=df[df['y'] == 1], x='age', label='Loan Taken')\n\nHistogram + hue\nsns.histplot(data=df, x='balance', hue='y', bins=30, kde=True, stat='density')\nüîπ 5. Dodatkowe / Interaktywne\n‚úÖ FacetGrid\nPodzielone wykresy np. wed≈Çug marital i education\ng = sns.FacetGrid(df, col=\"marital\", row=\"education\", hue=\"y\")\ng.map(sns.kdeplot, \"age\", fill=True)\n\njointplot (sns.jointplot)\nScatter + marginesowe rozk≈Çady\nsns.jointplot(data=df, x='balance', y='duration', hue='y', kind='kde')\n\n","metadata":{}},{"cell_type":"markdown","source":"## 6. Detailed check of categorical columns properties","metadata":{}},{"cell_type":"code","source":"sns.catplot(\n    data=train_df[train_df['Personality'].isin(['Introvert', 'Extrovert'])],  \n    x=\"Stage_fear\",\n    hue=\"Personality\",\n    col=\"Drained_after_socializing\",\n    kind=\"count\",\n    height=4,\n    aspect=1\n)\nplt.suptitle(\"Rozk≈Çad StageFear wg osobowo≈õci\", y=1.05)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.374186Z","iopub.execute_input":"2025-08-03T09:16:31.374520Z","iopub.status.idle":"2025-08-03T09:16:31.615904Z","shell.execute_reply.started":"2025-08-03T09:16:31.374495Z","shell.execute_reply":"2025-08-03T09:16:31.614474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.crosstab([train_df['Personality'],train_df[\"Stage_fear\"]],train_df[\"Drained_after_socializing\"]  )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.616847Z","iopub.status.idle":"2025-08-03T09:16:31.617286Z","shell.execute_reply.started":"2025-08-03T09:16:31.617063Z","shell.execute_reply":"2025-08-03T09:16:31.617082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- Stage_fear and Drained_after_socializing columns seems to be strong indicator of classification\n- In each column there is small representation of opposite group prefference (Extrovert preffers No & No, but there is small group of introverts with same prefferences)\n- It is very rare situation to have Yes - No and No - Yes answers. It is very specific minor group\n- It may be resonable to impute data Yes when other column is Yes and oposite for No, for such rare cases\n","metadata":{}},{"cell_type":"markdown","source":"####  Below we check what would be result of imputation Yes to Yes and No to No for both ways","metadata":{}},{"cell_type":"code","source":"cat_cols1 = ['Stage_fear','Drained_after_socializing']\nall_train_df=train_df[['Stage_fear','Drained_after_socializing','Personality']].copy()\nall_train_df[cat_cols1]=all_train_df[cat_cols1].fillna('Missing').astype(str)\ndisplay(pd.crosstab([all_train_df['Personality'],all_train_df[\"Stage_fear\"]],all_train_df[\"Drained_after_socializing\"]  ))\n\n\nhelp_train_df = train_df[['Stage_fear','Drained_after_socializing','Personality']].copy()\nhelp_train_df['Stage_fear'] = help_train_df['Stage_fear'].mask(help_train_df['Stage_fear'].isna() & help_train_df['Drained_after_socializing']\n                                            .notna(), help_train_df['Drained_after_socializing'])\nhelp_train_df['Drained_after_socializing'] = help_train_df['Drained_after_socializing'].mask(help_train_df['Drained_after_socializing']\n                                            .isna() & help_train_df['Stage_fear'].notna(), help_train_df['Stage_fear'])\nhelp_train_df[cat_cols1]=help_train_df[cat_cols1].fillna('Missing').astype(str)\n\ndisplay(pd.crosstab(help_train_df['Stage_fear'],help_train_df['Personality']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.618551Z","iopub.status.idle":"2025-08-03T09:16:31.618847Z","shell.execute_reply.started":"2025-08-03T09:16:31.618712Z","shell.execute_reply":"2025-08-03T09:16:31.618724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- There is only 39 missing values for both categorical columns and for total 18524 rows it seems to be a good imputation stratego - to be tested","metadata":{}},{"cell_type":"markdown","source":"## 7. Detailed check of No-No / Yes-Yes data regarding Introverts & Extroverts groups","metadata":{}},{"cell_type":"code","source":"train_introvert_df = train_df[train_df['Personality']=='Introvert']\ntrain_extrovert_df = train_df[train_df['Personality']=='Extrovert']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"introvert_no_no_df = train_df[(train_df[\"Stage_fear\"]=='No') & (train_df[\"Drained_after_socializing\"]=='No') & (train_df['Personality']=='Introvert')]\nextrovert_yes_yes_df = train_df[(train_df[\"Stage_fear\"]=='Yes') & (train_df[\"Drained_after_socializing\"]=='Yes') & (train_df['Personality']=='Extrovert')]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in columns:\n    if train_df[col].dtype in[np.int64,np.float64]:\n        sns.kdeplot(introvert_no_no_df[col], label='introvert with no-no', fill=True)\n        sns.kdeplot(train_introvert_df[col], label='train_df introvert', fill=True)\n        sns.kdeplot(train_extrovert_df[col], label='train_df extrovert', fill=True)\n        plt.legend()\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in columns:\n    if train_df[col].dtype in[np.int64,np.float64]:\n        sns.kdeplot(extrovert_yes_yes_df[col], label='extrovert with yes-yes', fill=True)\n        sns.kdeplot(train_introvert_df[col], label='train_df introvert', fill=True)\n        sns.kdeplot(train_extrovert_df[col], label='train_df extrovert', fill=True)\n        plt.legend()\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in columns:\n    if train_df[col].dtype in[np.int64,np.float64]:\n        sns.kdeplot(extrovert_yes_yes_df[col], label='extrovert with yes-yes', fill=True)\n        sns.kdeplot(train_introvert_df[col], label='train_df introvert', fill=True)\n        sns.kdeplot(train_extrovert_df[col], label='train_df extrovert', fill=True)\n        sns.kdeplot(introvert_no_no_df[col], label='introvert with no-no', fill=True)\n        plt.legend()\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- It seems like main problem of this classification. No-No answer is typical for Extroverts but some Introverts have same prefferences like No-No and has similar values for other colums (Post_frequency, Going_outsice...) like Extroverts\n- It will be extremly hard for classifiers to deal with such situation. There is some space in overlaping area between yes-yes and no-no groups and it can be marked with 0 and 1 giving clear information to model that is border condition. - Can be tested ","metadata":{}},{"cell_type":"code","source":"for col in test_df.columns:\n    if col != 'id' and test_df[col].dtype in[np.int64,np.float64]:\n        sns.kdeplot(introvert_no_no_df[col], label='introvert with no-no', fill=True)\n        sns.kdeplot(extrovert_yes_yes_df[col], label='extrovert with yes-yes', fill=True)\n\n        plt.legend()\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Markers to be tested:\n- 'Time_spent_Alone' == 4\n- 'Social_event_attendance' == 3\n- 'Going_outside' ==3\n- 'Friends_circle_size' == 5\n- 'Post_frequency' == 3","metadata":{"execution":{"iopub.status.busy":"2025-07-30T07:33:32.890719Z","iopub.execute_input":"2025-07-30T07:33:32.891163Z","iopub.status.idle":"2025-07-30T07:33:32.897390Z","shell.execute_reply.started":"2025-07-30T07:33:32.891135Z","shell.execute_reply":"2025-07-30T07:33:32.895495Z"}}},{"cell_type":"markdown","source":"## 8. Missing values - looking for signals","metadata":{}},{"cell_type":"code","source":"excluded_cols = ['id', 'Personality']\nall_columns = train_df.columns\nfor col in all_columns:\n    if col not in excluded_cols:\n        train_df[col + '_MISS'] = train_df[col].notna().astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = ['Time_spent_Alone_MISS','Stage_fear_MISS', 'Social_event_attendance_MISS', 'Going_outside_MISS','Drained_after_socializing_MISS', \n           'Friends_circle_size_MISS','Post_frequency_MISS']\nfor col in columns:\n    train_df.groupby([col,'Personality']).size().unstack().plot(kind='bar', stacked=True, title=col)\n    result = pd.crosstab(train_df[col],train_df['Personality'], normalize='index')*100\n    chi2, p, _, _ = chi2_contingency(result)\n    print(f\"Chi2 = {chi2:.3f}, p-value = {p:.4f} Column: {col} \")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- Columns Stage_fear_MISS and Drained_after_socializing_MISS have p-value lower then 0.05 so they can be considered as potential signal for different distribution of Introverts/Extroverts","metadata":{}},{"cell_type":"markdown","source":"## 9. Missing values - total number of non-missing data","metadata":{}},{"cell_type":"code","source":"train_df['not_MISS_total'] = train_df[columns].sum(axis=1)\ntrain_df.groupby(['not_MISS_total','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Data without NaN values')\npd.crosstab(train_df['not_MISS_total'],train_df['Personality'], normalize='index')*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- When number of missing values for one person increase it is observed that percentage of introverts in such grup increases too. ","metadata":{}},{"cell_type":"markdown","source":"## 10. Advanced Data Imputation ","metadata":{}},{"cell_type":"code","source":"num_cols = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size','Post_frequency']    \ncat_cols = ['Stage_fear', 'Drained_after_socializing']     \n\ncat_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n\ntrain_df[cat_cols] = cat_encoder.fit_transform(train_df[cat_cols])\nnum_imputer = IterativeImputer(estimator=LGBMRegressor(n_estimators=500, learning_rate=0.03, max_depth=6, subsample=0.8, colsample_bytree=0.8, verbosity=-1),\n                               max_iter=10, random_state=42)\n\ntrain_df[num_cols] = num_imputer.fit_transform(train_df[num_cols])\ncat_imputer = IterativeImputer(estimator=LGBMClassifier(n_estimators=500, learning_rate=0.03, max_depth=6, subsample=0.8, colsample_bytree=0.8, class_weght='balanced', verbosity=-1),\n                               max_iter=10, random_state=42)\n\ntrain_df[cat_cols] = cat_imputer.fit_transform(train_df[cat_cols])\n\ncolumns = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size','Post_frequency']\ntrain_df[columns]=train_df[columns].round().astype(int)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 11. Realations check between columns","metadata":{"execution":{"iopub.status.busy":"2025-07-26T14:48:08.993241Z","iopub.execute_input":"2025-07-26T14:48:08.993576Z","iopub.status.idle":"2025-07-26T14:48:08.997773Z","shell.execute_reply.started":"2025-07-26T14:48:08.993554Z","shell.execute_reply":"2025-07-26T14:48:08.996803Z"}}},{"cell_type":"markdown","source":"### Time_spent_Alone / Going_outside","metadata":{"execution":{"iopub.status.busy":"2025-07-26T16:07:28.446182Z","iopub.execute_input":"2025-07-26T16:07:28.446498Z","iopub.status.idle":"2025-07-26T16:07:28.451114Z","shell.execute_reply.started":"2025-07-26T16:07:28.446474Z","shell.execute_reply":"2025-07-26T16:07:28.449714Z"}}},{"cell_type":"code","source":"train_df['Time_Alone_dev_Outside'] = train_df['Time_spent_Alone'] / train_df['Going_outside']\ntrain_df['Time_Alone_dev_Outside']=train_df['Time_Alone_dev_Outside'].round(2).astype(float)\ntrain_df.groupby(['Time_Alone_dev_Outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Time_Alone_dev_Outside (x):\n    try:\n        x=float(x)\n        if x <= 1:\n            return 0\n        elif x > 1 and x < 2:\n            return 1\n        elif x >= 2 and x < 100:\n            return 2\n        else:\n            return 3\n    except ValueError:\n        return 3\n\ntrain_df['Time_Alone_dev_Outside']=train_df['Time_Alone_dev_Outside'].apply(Time_Alone_dev_Outside).astype('Int64')\ntrain_df.groupby(['Time_Alone_dev_Outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\npd.crosstab(train_df[\"Time_Alone_dev_Outside\"],train_df['Personality'],normalize='index')*100 ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- If columns Time_spent_Alone / Going_outside are devided we can observed quite good separation of data\n- To reduce number of features we can use clustering or write simple function, each group have different ratio\n- Can be check as additional feature to recognise groups","metadata":{"execution":{"iopub.status.busy":"2025-07-30T13:50:14.224526Z","iopub.execute_input":"2025-07-30T13:50:14.224816Z","iopub.status.idle":"2025-07-30T13:50:14.230744Z","shell.execute_reply.started":"2025-07-30T13:50:14.224796Z","shell.execute_reply":"2025-07-30T13:50:14.229624Z"}}},{"cell_type":"markdown","source":"## Social_event_attendance / Post_frequency","metadata":{}},{"cell_type":"code","source":"train_df['Social_dev_Post'] = train_df['Social_event_attendance'] / train_df['Post_frequency']\ntrain_df['Social_dev_Post']=train_df['Social_dev_Post'].round(2).astype(float)\ntrain_df.groupby(['Social_dev_Post','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Alternatywne grupowanie\ntrain_df['Social_dev_Post'] = train_df['Social_event_attendance'] / train_df['Post_frequency']\ntrain_df['Social_dev_Post']=train_df['Social_dev_Post'].round(2).astype(float)\n\ndef Social_dev_Post (x):\n    try:\n        x=float(x)\n        if x == 0:\n            return 0\n        elif x > 0 and x < 0.33:\n            return 1\n        elif x == 0.33:\n            return 2\n        elif x > 0.33 and x < 0.5:\n            return 1\n        elif x == 0.5:\n            return 4\n        elif x > 0.5 and x < 0.67:\n            return 1\n        elif x == 0.67:\n            return 4\n        elif x > 0.67 and x < 1:\n            return 1\n        elif x == 1:\n            return 4\n        elif x > 1 and x < 1.5:\n            return 1\n        elif x == 1.5:\n            return 4\n        elif x > 1.5 and x < 2:\n            return 1\n        elif x == 2:\n            return 4\n        elif x > 2 and x < 3:\n            return 1\n        elif x == 3:\n            return 2\n        elif x > 3 and x < 100:\n            return 1\n        else:\n            return 0\n    except ValueError:\n        return 0\n\ntrain_df['Social_dev_Post']=train_df['Social_dev_Post'].apply(Social_dev_Post).astype('Int64')\ntrain_df.groupby(['Social_dev_Post','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\npd.crosstab(train_df[\"Social_dev_Post\"],train_df['Personality'], normalize='index')*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- If columns Social_event_attendance / Post_frequency are devided we can observed quite good separation of data in fixed points\n- To reduce number of features we can use clustering or write simple function, each group have different ratio\n- Can be check as additional feature to recognise groups","metadata":{}},{"cell_type":"markdown","source":"## Going_outside * Friends_circle_size","metadata":{}},{"cell_type":"code","source":"train_df['Outside_mult_Friends'] = train_df['Going_outside'] * train_df['Friends_circle_size']\ntrain_df.groupby(['Outside_mult_Friends','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Outside_mult_Friends (x):\n    try:\n        x=float(x)\n        if x <= 11:\n            return 0\n        elif x > 11 and x <= 15:\n            return 1\n        elif x > 15 and x < 400:\n            return 2\n        else:\n            return 2\n    except ValueError:\n        return 2\n\ntrain_df['Outside_mult_Friends']=train_df['Outside_mult_Friends'].apply(Outside_mult_Friends).astype('Int64')\ntrain_df.groupby(['Outside_mult_Friends','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\npd.crosstab(train_df[\"Outside_mult_Friends\"],train_df['Personality'], normalize='index')*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- If columns Going_outside * Friends_circle_size are multiplied we can observed quite good separation of data \n- To reduce number of features we can use clustering or write simple function, each group have different ratio\n- Can be check as additional feature to recognise groups","metadata":{}},{"cell_type":"markdown","source":"## Going_outside - Post_frequency","metadata":{}},{"cell_type":"code","source":"train_df['Going_sub_Post']=train_df['Going_outside'] - train_df['Post_frequency']\ntrain_df.groupby(['Going_sub_Post','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Outside_mult_Friends (x):\n    try:\n        x=float(x)\n        if x <= -6:\n            return 0\n        elif x == -5 or x== -4 or x==4:\n            return 1\n        elif x == -3 or x== 3:\n            return 2\n        elif x == -2 or x== 2:\n            return 3\n        elif x == -1 or x== 0 or x==1:\n            return 4\n        elif x >= 5:\n            return 5\n        else:\n            return 6\n    except ValueError:\n        return 6\n\ntrain_df['Going_sub_Post']=train_df['Going_sub_Post'].apply(Outside_mult_Friends).astype('Int64')\ntrain_df.groupby(['Going_sub_Post','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\npd.crosstab(train_df[\"Going_sub_Post\"],train_df['Personality'], normalize='index')*100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- If columns Going_outside - Post_frequency are subtracted from each other we can observed quite good separation of data \n- To reduce number of features we can use clustering or write simple function, each group have different ratio\n- Can be check as additional feature to recognise groups\n- group 0 is still not pure group","metadata":{}},{"cell_type":"markdown","source":"## 12. Realations check between columns - Other ","metadata":{}},{"cell_type":"markdown","source":"## Columns subtraction","metadata":{}},{"cell_type":"code","source":"train_df['subtraction']=train_df['Friends_circle_size'] - train_df['Post_frequency']\ntrain_df.groupby(['subtraction','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['subtraction']=train_df['Going_outside'] - train_df['Post_frequency']\ntrain_df.groupby(['subtraction','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['subtraction']=train_df['Going_outside'] - train_df['Friends_circle_size']\ntrain_df.groupby(['subtraction','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.650542Z","iopub.status.idle":"2025-08-03T09:16:31.650817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['subtraction']=train_df['Social_event_attendance'] - train_df['Post_frequency']\ntrain_df.groupby(['subtraction','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.652019Z","iopub.status.idle":"2025-08-03T09:16:31.652373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['subtraction']=train_df['Social_event_attendance'] - train_df['Friends_circle_size']\ntrain_df.groupby(['subtraction','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.653550Z","iopub.status.idle":"2025-08-03T09:16:31.653817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['subtraction']=train_df['Social_event_attendance'] - train_df['Going_outside']\ntrain_df.groupby(['subtraction','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.654668Z","iopub.status.idle":"2025-08-03T09:16:31.654901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['subtraction']=train_df['Time_spent_Alone'] - train_df['Post_frequency']\ntrain_df.groupby(['subtraction','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.655984Z","iopub.status.idle":"2025-08-03T09:16:31.656286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['subtraction']=train_df['Time_spent_Alone'] - train_df['Friends_circle_size']\ntrain_df.groupby(['subtraction','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.657809Z","iopub.status.idle":"2025-08-03T09:16:31.658410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['subtraction']=train_df['Time_spent_Alone'] - train_df['Going_outside']\ntrain_df.groupby(['subtraction','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.659177Z","iopub.status.idle":"2025-08-03T09:16:31.659501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['subtraction']=train_df['Time_spent_Alone'] - train_df['Social_event_attendance']\ntrain_df.groupby(['subtraction','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.661175Z","iopub.status.idle":"2025-08-03T09:16:31.661497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Column summing","metadata":{}},{"cell_type":"code","source":"train_df['Summary']=train_df['Friends_circle_size'] + train_df['Post_frequency']\ntrain_df.groupby(['Summary','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.662813Z","iopub.status.idle":"2025-08-03T09:16:31.663234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Summary']=train_df['Going_outside'] + train_df['Friends_circle_size']\ntrain_df.groupby(['Summary','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.664303Z","iopub.status.idle":"2025-08-03T09:16:31.664600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Summary']=train_df['Going_outside'] + train_df['Post_frequency']\ntrain_df.groupby(['Summary','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.666119Z","iopub.status.idle":"2025-08-03T09:16:31.666470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Summary']=train_df['Social_event_attendance'] + train_df['Post_frequency']\ntrain_df.groupby(['Summary','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.667580Z","iopub.status.idle":"2025-08-03T09:16:31.667880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Summary']=train_df['Social_event_attendance'] + train_df['Going_outside']\ntrain_df.groupby(['Summary','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.668705Z","iopub.status.idle":"2025-08-03T09:16:31.669025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Other column multiplications and divisions","metadata":{}},{"cell_type":"code","source":"train_df['Time_alona_outside'] = train_df['Going_outside'] / train_df['Post_frequency']\ntrain_df.groupby(['Time_alona_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\ntrain_df['Time_alona_outside'] = train_df['Social_event_attendance'] / train_df['Post_frequency']\ntrain_df.groupby(['Time_alona_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\ntrain_df['Time_alona_outside'] = train_df['Time_spent_Alone'] / train_df['Post_frequency']\ntrain_df.groupby(['Time_alona_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\ntrain_df['Time_alona_outside'] = train_df['Time_spent_Alone'] / train_df['Friends_circle_size']\ntrain_df.groupby(['Time_alona_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\ntrain_df['Time_alona_outside'] = train_df['Time_spent_Alone'] / train_df['Social_event_attendance']\ntrain_df.groupby(['Time_alona_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\ntrain_df['Time_alona_outside'] = train_df['Time_spent_Alone'] / train_df['Going_outside']\ntrain_df.groupby(['Time_alona_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\ntrain_df['Time_alone_friends_circle'] = train_df['Time_spent_Alone'] / train_df['Friends_circle_size']\ntrain_df.groupby(['Time_alone_friends_circle','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\n\ntrain_df['Social_event_outside'] = train_df['Going_outside'] * train_df['Social_event_attendance']\ntrain_df.groupby(['Social_event_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\ntrain_df['Social_event_outside'] = train_df['Social_event_attendance'] * train_df['Friends_circle_size']\ntrain_df.groupby(['Social_event_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\ntrain_df['Social_event_outside'] = train_df['Social_event_attendance'] * train_df['Post_frequency']\ntrain_df.groupby(['Social_event_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\ntrain_df['Social_event_outside'] = train_df['Going_outside'] * train_df['Friends_circle_size']\ntrain_df.groupby(['Social_event_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\n\ntrain_df['Social_event_outside'] = train_df['Going_outside'] * train_df['Post_frequency']\ntrain_df.groupby(['Social_event_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n\ntrain_df['Social_event_outside'] = train_df['Friends_circle_size'] * train_df['Post_frequency']\ntrain_df.groupby(['Social_event_outside','Personality']).size().unstack().plot(kind='bar', stacked=True, title='Summary')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:16:31.670158Z","iopub.status.idle":"2025-08-03T09:16:31.670537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Observations:\n- For checked interactions there is no pure separation between Extroverts and Introverts\n- Some additional features can be created to be tested, if there is improvement in classification\n","metadata":{}}]}